output {
  if [@metadata][beat] == "heartbeat" {

    # Useful for debugging
    # stdout { codec => line { format => "%{[monitor][id]} status: %{[monitor][status]} / Previous: %{[@metadata][previous_status]} / Tags: %{[tags]}" } }

    # Only alert if the event is not marked as throttled
    if "throttle_slack_alert" not in [tags] {

      if [monitor][status] == "down" {
        # Up --> Down - Alert monitor is down
        if [@metadata][previous_status] != "down" {

          http {
            http_method => "post"
            url => "https://hooks.slack.com/services/${SLACK_ALERTS_WEBHOOK}"
            format => "message"
            message => '{"text": ":fire: %{[monitor][name]} appears to be down","blocks": [{"type": "section","text": {"type": "mrkdwn","text": ":fire: *<%{[url][full]}|%{[monitor][name]}>* appears to be *down*\n> %{[error][message]}"}},{"type": "context","elements": [{"type": "mrkdwn","text": "For more info, open <${KIBANA_UPTIME_URL}|Kibana Uptime>"}]}]}'
          }

        } else {
          # TODO: Add repeat alert about down-ness -- e.g. if persistently down for 10 minutes.
        }
      } else {
        # Down --> Up - Alert monitor has come back up
        if [@metadata][previous_status] == "down" {

          http {
            http_method => "post"
            url => "https://hooks.slack.com/services/${SLACK_ALERTS_WEBHOOK}"
            format => "message"
            message => '{"text": ":ok_hand: %{[monitor][name]} came back up","blocks": [{"type": "section","text": {"type": "mrkdwn","text": ":ok_hand: *<%{[url][full]}|%{[monitor][name]}>* came back *up*"}},{"type": "context","elements": [{"type": "mrkdwn","text": "For more info, open <${KIBANA_UPTIME_URL}|Kibana Uptime>"}]}]}'
          }

        }
      }
    }
  }
}
